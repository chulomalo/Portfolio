{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f0642aa",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch Applied to Root Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cf8a34",
   "metadata": {},
   "source": [
    "This project focuses on building a neural network from scratch to solve the root-finding problem for quadratic equations. The goal is to accurately predict the real roots of these equations using an optimized neural network architecture. Key features include modular design, performance optimization, and flexibility, enabling the network to tackle root-finding tasks with precision and efficiency.\n",
    "\n",
    "By combining innovative techniques and a streamlined structure, this model is designed to handle quadratic root predictions with ease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f8c16",
   "metadata": {},
   "source": [
    "## Step 1: Modular Neural Network Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dbd8cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, dim_in, dim_out, activation=None):\n",
    "        self.weights = 0.1 * np.random.randn(dim_in, dim_out)\n",
    "        self.biases = np.zeros((1, dim_out))\n",
    "        self.activation = activation\n",
    "        self.momentum_weights = np.zeros_like(self.weights)\n",
    "        self.cache_weights = np.zeros_like(self.weights)\n",
    "        self.momentum_biases = np.zeros_like(self.biases)\n",
    "        self.cache_biases = np.zeros_like(self.biases)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "        if self.activation == 'relu':\n",
    "            self.activation_outputs = np.maximum(0, self.outputs)\n",
    "        else:\n",
    "            self.activation_outputs = self.outputs\n",
    "        return self.activation_outputs\n",
    "\n",
    "    def backward(self, grad):\n",
    "        if self.activation == 'relu':\n",
    "            grad[self.outputs <= 0] = 0\n",
    "        self.grad_weights = np.dot(self.inputs.T, grad)\n",
    "        self.grad_biases = np.sum(grad, axis=0, keepdims=True)\n",
    "        return np.dot(grad, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c60845",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "We encapsulated the neural network layer in a `Layer` class, making it easier to manage and reuse. Each layer can perform a forward pass with optional ReLU activation, and backpropagate gradients for updating weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56a63ac",
   "metadata": {},
   "source": [
    "## Step 2: The Optimizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd85f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerAdam:\n",
    "    def __init__(self, learning_rate=0.01, decay=1e-3, beta1=0.9, beta2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iterations = 0\n",
    "\n",
    "    def update(self, layer):\n",
    "        if not hasattr(layer, 'momentum_weights'):\n",
    "            return\n",
    "        layer.momentum_weights = self.beta1 * layer.momentum_weights + (1 - self.beta1) * layer.grad_weights\n",
    "        layer.momentum_biases = self.beta1 * layer.momentum_biases + (1 - self.beta1) * layer.grad_biases\n",
    "\n",
    "        corrected_momentum_weights = layer.momentum_weights / (1 - self.beta1 ** (self.iterations + 1))\n",
    "        corrected_momentum_biases = layer.momentum_biases / (1 - self.beta1 ** (self.iterations + 1))\n",
    "\n",
    "        layer.cache_weights = self.beta2 * layer.cache_weights + (1 - self.beta2) * layer.grad_weights ** 2\n",
    "        layer.cache_biases = self.beta2 * layer.cache_biases + (1 - self.beta2) * layer.grad_biases ** 2\n",
    "\n",
    "        corrected_cache_weights = layer.cache_weights / (1 - self.beta2 ** (self.iterations + 1))\n",
    "        corrected_cache_biases = layer.cache_biases / (1 - self.beta2 ** (self.iterations + 1))\n",
    "\n",
    "        lr = self.learning_rate * (1 / (1 + self.decay * self.iterations))\n",
    "\n",
    "        layer.weights += -lr * corrected_momentum_weights / (np.sqrt(corrected_cache_weights) + 1e-8)\n",
    "        layer.biases += -lr * corrected_momentum_biases / (np.sqrt(corrected_cache_biases) + 1e-8)\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30642e4c",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "Here, we introduce the Adam optimizer as a class, which manages learning rate decay and tracks momentum and cache for weight updates. This makes our code more modular and allows easy substitution with other optimizers if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca77916",
   "metadata": {},
   "source": [
    "## Step 3: Data Generation for Quadratic Root Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a911259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quadratic_data(samples):\n",
    "    X, y = [], []\n",
    "    for _ in range(samples):\n",
    "        a = np.random.uniform(-100, 100)\n",
    "        b = np.random.uniform(-100, 100)\n",
    "        c = np.random.uniform(-100, 100)\n",
    "        roots = np.roots([a, b, c])\n",
    "        real_roots = [root.real for root in roots if root.imag == 0]\n",
    "        if real_roots:\n",
    "            X.append([a, b, c])\n",
    "            y.append([max(real_roots)])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = generate_quadratic_data(3000)\n",
    "X_test, y_test = generate_quadratic_data(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe119ec3",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "In this step, we generate synthetic data for training and testing. Each data point represents the coefficients of a quadratic equation and the largest real root of the equation as the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e452be78",
   "metadata": {},
   "source": [
    "## Step 4: Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c153b021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 296.5527\n",
      "Epoch 100, Loss: 94.0541\n",
      "Epoch 200, Loss: 11.6159\n",
      "Epoch 300, Loss: 1.4288\n",
      "Epoch 400, Loss: 0.6062\n",
      "Epoch 500, Loss: 0.4476\n",
      "Epoch 600, Loss: 0.3722\n",
      "Epoch 700, Loss: 0.3309\n",
      "Epoch 800, Loss: 0.2996\n",
      "Epoch 900, Loss: 0.2739\n",
      "Epoch 1000, Loss: 0.2497\n",
      "Epoch 1100, Loss: 0.2324\n",
      "Epoch 1200, Loss: 0.2196\n",
      "Epoch 1300, Loss: 0.2073\n",
      "Epoch 1400, Loss: 0.1969\n",
      "Epoch 1500, Loss: 0.1879\n",
      "Epoch 1600, Loss: 0.1791\n",
      "Epoch 1700, Loss: 0.1706\n",
      "Epoch 1800, Loss: 0.1632\n",
      "Epoch 1900, Loss: 0.1562\n",
      "Epoch 2000, Loss: 0.1521\n",
      "Epoch 2100, Loss: 0.1439\n",
      "Epoch 2200, Loss: 0.1375\n",
      "Epoch 2300, Loss: 0.1312\n",
      "Epoch 2400, Loss: 0.1322\n",
      "Epoch 2500, Loss: 0.1189\n",
      "Epoch 2600, Loss: 0.1608\n",
      "Epoch 2700, Loss: 0.1087\n",
      "Epoch 2800, Loss: 0.1068\n",
      "Epoch 2900, Loss: 0.1013\n",
      "Epoch 3000, Loss: 0.0969\n",
      "Epoch 3100, Loss: 0.0932\n",
      "Epoch 3200, Loss: 0.0897\n",
      "Epoch 3300, Loss: 0.0869\n",
      "Epoch 3400, Loss: 0.0844\n",
      "Epoch 3500, Loss: 0.0895\n",
      "Epoch 3600, Loss: 0.1233\n",
      "Epoch 3700, Loss: 0.0767\n",
      "Epoch 3800, Loss: 0.0804\n",
      "Epoch 3900, Loss: 0.0734\n",
      "Epoch 4000, Loss: 0.0732\n",
      "Epoch 4100, Loss: 0.0734\n",
      "Epoch 4200, Loss: 0.0760\n",
      "Epoch 4300, Loss: 0.0637\n",
      "Epoch 4400, Loss: 0.0646\n",
      "Epoch 4500, Loss: 0.0620\n",
      "Epoch 4600, Loss: 0.0632\n",
      "Epoch 4700, Loss: 0.0622\n",
      "Epoch 4800, Loss: 0.1423\n",
      "Epoch 4900, Loss: 0.0671\n",
      "Epoch 5000, Loss: 0.0676\n",
      "Epoch 5100, Loss: 0.0659\n",
      "Epoch 5200, Loss: 0.0503\n",
      "Epoch 5300, Loss: 0.0493\n",
      "Epoch 5400, Loss: 0.0499\n",
      "Epoch 5500, Loss: 0.0478\n",
      "Epoch 5600, Loss: 0.0531\n",
      "Epoch 5700, Loss: 0.0463\n",
      "Epoch 5800, Loss: 0.0564\n",
      "Epoch 5900, Loss: 0.0434\n",
      "Epoch 6000, Loss: 0.0460\n",
      "Epoch 6100, Loss: 0.0432\n",
      "Epoch 6200, Loss: 0.0410\n",
      "Epoch 6300, Loss: 0.0404\n",
      "Epoch 6400, Loss: 0.0397\n",
      "Epoch 6500, Loss: 0.0390\n",
      "Epoch 6600, Loss: 0.0394\n",
      "Epoch 6700, Loss: 0.0383\n",
      "Epoch 6800, Loss: 0.0369\n",
      "Epoch 6900, Loss: 0.0485\n",
      "Epoch 7000, Loss: 0.0359\n",
      "Epoch 7100, Loss: 0.0353\n",
      "Epoch 7200, Loss: 0.0348\n",
      "Epoch 7300, Loss: 0.0341\n",
      "Epoch 7400, Loss: 0.0364\n",
      "Epoch 7500, Loss: 0.0351\n",
      "Epoch 7600, Loss: 0.0341\n",
      "Epoch 7700, Loss: 0.0339\n",
      "Epoch 7800, Loss: 0.0319\n",
      "Epoch 7900, Loss: 0.0313\n",
      "Epoch 8000, Loss: 0.0312\n",
      "Epoch 8100, Loss: 0.0303\n",
      "Epoch 8200, Loss: 0.0305\n",
      "Epoch 8300, Loss: 0.0306\n",
      "Epoch 8400, Loss: 0.0292\n",
      "Epoch 8500, Loss: 0.0287\n",
      "Epoch 8600, Loss: 0.0297\n",
      "Epoch 8700, Loss: 0.0283\n",
      "Epoch 8800, Loss: 0.0278\n",
      "Epoch 8900, Loss: 0.0351\n",
      "Epoch 9000, Loss: 0.0269\n",
      "Epoch 9100, Loss: 0.0265\n",
      "Epoch 9200, Loss: 0.0264\n",
      "Epoch 9300, Loss: 0.0258\n",
      "Epoch 9400, Loss: 0.0256\n",
      "Epoch 9500, Loss: 0.0262\n",
      "Epoch 9600, Loss: 0.0249\n",
      "Epoch 9700, Loss: 0.0336\n",
      "Epoch 9800, Loss: 0.0244\n",
      "Epoch 9900, Loss: 0.0241\n"
     ]
    }
   ],
   "source": [
    "network = [Layer(3, 64, activation='relu'), Layer(64, 64, activation='relu'), Layer(64, 1)]\n",
    "optimizer = OptimizerAdam(learning_rate=0.01, decay=1e-3)\n",
    "\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    inputs = X_train\n",
    "    for layer in network:\n",
    "        inputs = layer.forward(inputs)\n",
    "    predictions = inputs\n",
    "\n",
    "    loss = np.mean((predictions - y_train) ** 2)\n",
    "    grad = (2 * (predictions - y_train)) / y_train.size\n",
    "\n",
    "    for layer in reversed(network):\n",
    "        grad = layer.backward(grad)\n",
    "\n",
    "    for layer in network:\n",
    "        optimizer.update(layer)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f170895c",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "We train the network using our new modular structure. Loss is calculated using Mean Squared Error (MSE), and gradients are backpropagated to update each layer's weights and biases via the Adam optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc3144",
   "metadata": {},
   "source": [
    "## Step 5: Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16781a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 93.1237\n"
     ]
    }
   ],
   "source": [
    "inputs = X_test\n",
    "for layer in network:\n",
    "    inputs = layer.forward(inputs)\n",
    "test_predictions = inputs\n",
    "test_loss = np.mean((test_predictions - y_test) ** 2)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d1bf9",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "Finally, we evaluate the trained network on the test set, printing the test loss. The model should be reasonably accurate in predicting the real roots of quadratic equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f55c8594",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution Real Numpy Test Set: Calculating the real roots using numpy's roots\n",
    "numpy_real_solutions = []\n",
    "for coefficients in X_test:\n",
    "    roots = np.roots(coefficients)\n",
    "    real_roots = [root.real for root in roots if root.imag == 0]\n",
    "    numpy_real_solutions.append(max(real_roots) if real_roots else None)\n",
    "\n",
    "# Filter None values for test set where no real roots were found\n",
    "filtered_y_test = []\n",
    "filtered_model_predictions = []\n",
    "filtered_numpy_solutions = []\n",
    "\n",
    "for y_true, model_pred, numpy_pred in zip(y_test, test_predictions, numpy_real_solutions):\n",
    "    if numpy_pred is not None:\n",
    "        filtered_y_test.append(y_true[0])\n",
    "        filtered_model_predictions.append(model_pred[0])\n",
    "        filtered_numpy_solutions.append(numpy_pred)\n",
    "\n",
    "filtered_y_test = np.array(filtered_y_test)\n",
    "filtered_model_predictions = np.array(filtered_model_predictions)\n",
    "filtered_numpy_solutions = np.array(filtered_numpy_solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d398ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True Solution</th>\n",
       "      <th>Model Solution</th>\n",
       "      <th>Numpy Solution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.647576</td>\n",
       "      <td>1.806480</td>\n",
       "      <td>1.647576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.463067</td>\n",
       "      <td>3.337222</td>\n",
       "      <td>3.463067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.631952</td>\n",
       "      <td>10.920216</td>\n",
       "      <td>15.631952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.171441</td>\n",
       "      <td>1.153938</td>\n",
       "      <td>1.171441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.915359</td>\n",
       "      <td>0.821571</td>\n",
       "      <td>0.915359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.090382</td>\n",
       "      <td>0.953225</td>\n",
       "      <td>1.090382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.620483</td>\n",
       "      <td>1.510341</td>\n",
       "      <td>1.620483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.282316</td>\n",
       "      <td>0.044199</td>\n",
       "      <td>0.282316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.405600</td>\n",
       "      <td>0.423949</td>\n",
       "      <td>0.405600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.020239</td>\n",
       "      <td>1.073542</td>\n",
       "      <td>1.020239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   True Solution  Model Solution  Numpy Solution\n",
       "0       1.647576        1.806480        1.647576\n",
       "1       3.463067        3.337222        3.463067\n",
       "2      15.631952       10.920216       15.631952\n",
       "3       1.171441        1.153938        1.171441\n",
       "4       0.915359        0.821571        0.915359\n",
       "5       1.090382        0.953225        1.090382\n",
       "6       1.620483        1.510341        1.620483\n",
       "7       0.282316        0.044199        0.282316\n",
       "8       0.405600        0.423949        0.405600\n",
       "9       1.020239        1.073542        1.020239"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Display Solution Real Model Test Set and Solution Real Numpy Test Set for comparison\n",
    "import pandas as pd\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'True Solution': filtered_y_test,\n",
    "    'Model Solution': filtered_model_predictions,\n",
    "    'Numpy Solution': filtered_numpy_solutions\n",
    "})\n",
    "\n",
    "comparison_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46938f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 1.3789\n",
      "Mean Squared Error (MSE): 93.1237\n",
      "Accuracy within tolerance of 0.1: 54.55%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(filtered_numpy_solutions, filtered_model_predictions)\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(filtered_numpy_solutions, filtered_model_predictions)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "\n",
    "# Calculate accuracy based on a tolerance threshold\n",
    "tolerance = 0.1  # Define a tolerance level for accuracy\n",
    "correct_predictions = np.abs(filtered_numpy_solutions - filtered_model_predictions) <= tolerance\n",
    "accuracy = np.mean(correct_predictions) * 100\n",
    "print(f\"Accuracy within tolerance of {tolerance}: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d93ffa",
   "metadata": {},
   "source": [
    "### 1. Mean Absolute Error (MAE): 1.3789\n",
    "The MAE indicates that, on average, the model’s predictions are about 1.38 units away from the actual solutions (as given by NumPy).  \n",
    "In the context of root finding, this means the predicted roots tend to have an average error of around 1.38. While this might be acceptable in some cases, it may be considered high depending on the scale and precision required.\n",
    "\n",
    "### 2. Mean Squared Error (MSE): 93.1237\n",
    "The MSE shows that the average of the squared errors is approximately 93.12.  \n",
    "This higher value, compared to MAE, suggests that there are some instances where the model's predictions deviate significantly from the true roots. MSE is particularly sensitive to larger errors, as it squares the differences, highlighting that some predictions are notably inaccurate.\n",
    "\n",
    "### 3. Accuracy within Tolerance of 0.1: 54.55%\n",
    "This metric indicates that 54.55% of the model's predictions fall within 0.1 units of the true root values.  \n",
    "An accuracy of about 54.55% within a very small tolerance suggests that slightly over half of the model's predictions are relatively close to the true values. However, it also indicates that nearly half of the predictions are more than 0.1 units away, which may be significant depending on the application.\n",
    "\n",
    "# Final thoughts \n",
    "\n",
    "The model is reasonably accurate, but there is room for improvement:\n",
    "- The MAE and MSE values imply that while the model is generally predicting in the correct range, it often makes errors that are non-negligible, as reflected in the relatively high MSE.\n",
    "- The accuracy within the tolerance suggests that the model achieves close predictions about half of the time, but might need more tuning, additional training data, or a more complex model architecture to improve its precision.\n",
    "\n",
    "If more accuracy is required, consider further training with additional data or tuning the network parameters to improve performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
